# Moshi

I've recently made a knowledge sharing session about Moshi, a speech-text foundation model. My goal with this presentation was to explore some of the ideas presented in the [paper](https://arxiv.org/abs/2410.00037), so that people could understand the components needed to build a speech-to-speech model. The video was recorded and I published it on Youtube (see below) with all of the participant's video and audio removed.

By watching the video, you’ll understand

- How Mimi allows Moshi to “hear” and “speak”
- Why the RQ-transformer architecture can be of help to model audio and how it works
- What is the “Inner Monologue” method and how it significantly improves the linguistic quality of Moshi’s generated speech

[![MOSHI KSS](https://i9.ytimg.com/vi/5X2F85Vrj7o/mq2.jpg?sqp=CLjzqcgG-oaymwEmCMACELQB8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgZShlMA8=&rs=AOn4CLDhRS5RL87yvtjjqZ8KvHfHPDD_Fg)](https://youtu.be/5X2F85Vrj7o)
