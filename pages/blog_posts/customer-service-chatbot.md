# Why does every company seem to be creating a customer service chatbot?

## Introduction

This is an opinion article based on my experience as a customer and developer. I’m writing this because I want people to comment it out and ignite the debate around it. Bear in mind this is simply a snapshot of my thoughts at the moment of writing and, as a lifelong learner, it is most likely they will change as I gather more data points (I’ll be really happy if you can contribute to it!).

So, chatbots. I must say that I’m getting tired of reading that every company seems to be releasing a chatbot to their website. Don’t get me wrong, I think the large language models (LLMs) boom is amazing. I use several LLM-based tools, like Gemini or GitHub Copilot, almost daily and they make some of my tasks so much easier. Gemini (or ChatGPT) is great for learning concepts that I’ve never heard about, as it can provide a simple explanation and intuitive examples – it’s really amazing that I can interact with it, learn from its responses, and formulate new questions based on what it tells me – and GitHub Copilot allows me to be more productive while coding.

Although these tools are incredible, I don’t get why there are so many companies building chatbots for customer service. To what extent are they actually useful to the customers?

## Customer & developer views

As a customer, for most of the products/services I use, a chatbot won’t be of any help. If I have doubts about a particular feature or on how to do something I’ll ask Google. In turn, Google will typically provide me with the company’s page that answers my question in the first search results. If Google fails to lead me to the correct page (which normally doesn’t), I can just go to the company’s website and search (or even go to other websites where users have the same problem and share what they’ve done). Most modern websites are easy to navigate and the information is quite accessible (if it isn’t, then I don’t think the priority should be building chatbots).

In the extreme case when I don’t find the information, it’s because a) it does not exist on the website or b) is purposefully hidden. But if it does not exist on the website, a chatbot should also not be able to answer my question (at least I think it does not make much sense to have a chatbot that answers about a general topic that is not exposed in the website). And if it is hidden on purpose (such as how to make complaints), then the chatbot won’t, most likely, answer with useful information. It will try to dissuade you (which will make clients go mad – at least me).

Moreover, as a developer I am able to see what’s under the hood of a customer service chatbot and, as far as I know, most of them use a technique called retrieval augmented generation (RAG). Essentially, since large language models (the backbone of a chatbot) are trained on a fixed time range of data, they won’t be able to respond with up to date information, or to know information that is only available in a company’s database. RAG enables LLMs to answer with up to date information by searching the relevant documents to respond to a given question and telling the LLM to answer based on it. There are many variations of this technique, but the basic principle is the one I’ve just described. This seems great, right? You overcome a shortcoming of LLMs relatively easy and provide up to date information.

The problem is that LLMs tend to hallucinate (they answer with incorrect information) and this is still an [open research question](https://huyenchip.com/2023/08/16/llm-research-open-challenges.html). From my experience that tends to happen more often when you provide it with information that does not contain the necessary content to answer the question. And this happens even if you tell it to respond that it does not know when the information is not sufficient to answer the question. As an example, I’ve used GPT 3.5 turbo to answer how to subscribe to HBO, but only provided it with information about how to subscribe to Netflix and Disney+. What happened? It responded wrongly on how I could subscribe to HBO. It picked parts of the Netflix and Disney+ contents and concocted a plausible answer that was not true.

The example I’ve just given has a low impact, though. However, there are several examples of chatbots that caused more serious problems. Take, for instance, the [DPD customer service chatbot that swore and called the company “worst delivery firm”](https://news.sky.com/story/dpd-customer-service-chatbot-swears-and-calls-company-worst-delivery-service-13052037#:~:text=DPD%20has%20disabled%20its%20artificial,criticising%20the%20parcel%20delivery%20company.). Or the [NYC’s AI chatbot which was caught telling businesses to break the law](https://apnews.com/article/new-york-city-chatbot-misinformation-6ebc71db5b770b9969c906a7ee4fae21). Or the [Watsonville Chevy dealership’s chatbot which offered to sell a guy a 2024 Chevy Tahoe for a dollar](https://gizmodo.com/ai-chevy-dealership-chatgpt-bot-customer-service-fail-1851111825).

## Final Remarks

I believe such situations can cause clients to lose trust in a company and degrade its reputation. So, why do companies put themselves in such situations? Were they deceived and led into believing that LLMs are more robust than they actually are? Or are they well aware of the risks, but still want to launch such services? If so, is it to gather more customer data? Is it for fear of missing out the generative AI train? And for what? To build another customer service chatbot that is not particularly useful? I honestly don’t know the answer to this questions, but I urge you to leave a comment so that I can start to answer such questions and, perhaps, start to see things from a different perspective.
